{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0yAE1nLUzNoG6J9RNtpkl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saisiridasari/Portfolio-new/blob/main/Hate_Speech_Identifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFwHJZvsQGrM",
        "outputId": "1f0c0b05-0467-4392-d64b-30a8b26b1d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.62      0.43       286\n",
            "           1       0.97      0.85      0.91      3838\n",
            "           2       0.76      0.95      0.84       833\n",
            "\n",
            "    accuracy                           0.85      4957\n",
            "   macro avg       0.68      0.81      0.72      4957\n",
            "weighted avg       0.90      0.85      0.87      4957\n",
            "\n",
            "Accuracy: 0.8525\n",
            "\n",
            "Sample Predictions:\n",
            "Text: I hope you have a great day! -> Prediction: {'label': 'Neither', 'confidence': 0.8104424493789826}\n",
            "Text: Let’s meet up for lunch tomorrow. -> Prediction: {'label': 'Neither', 'confidence': 0.6595426366726486}\n",
            "Text: You're such an idiot, get lost! -> Prediction: {'label': 'Offensive', 'confidence': 0.37771679702426164}\n",
            "Text: Shut up already, no one cares! -> Prediction: {'label': 'Offensive', 'confidence': 0.4978445162482476}\n",
            "Text: All [ethnic group] are criminals! -> Prediction: {'label': 'Neither', 'confidence': 0.7562528684033519}\n",
            "Text: I hate all [religion] people, they're disgusting. -> Prediction: {'label': 'Hate Speech', 'confidence': 0.48220313986668467}\n",
            "Text: People like you ruin this country! -> Prediction: {'label': 'Hate Speech', 'confidence': 0.5233837036543756}\n",
            "Text: Congratulations on your achievement! -> Prediction: {'label': 'Neither', 'confidence': 0.5980401906515352}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import joblib\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Download necessary NLTK data\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Text Preprocessing Class\n",
        "class TextCleaner:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english')) - {'not', 'no'}\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = re.sub(r\"http\\S+|www\\S+|@\\w+\", \"\", text)\n",
        "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "        text = text.lower()\n",
        "        words = nltk.word_tokenize(text)\n",
        "        words = [self.lemmatizer.lemmatize(w) for w in words if w not in self.stop_words and len(w) > 1]\n",
        "        return \" \".join(words)\n",
        "\n",
        "\n",
        "# Load and Prepare Data\n",
        "\n",
        "def load_and_prepare_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    if 'tweet' not in df.columns or 'class' not in df.columns:\n",
        "        raise ValueError(\"CSV must contain 'tweet' and 'class' columns\")\n",
        "\n",
        "    df = df.dropna(subset=['tweet'])\n",
        "\n",
        "    cleaner = TextCleaner()\n",
        "    df['clean_text'] = df['tweet'].apply(cleaner.clean_text)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
        "    X = vectorizer.fit_transform(df['clean_text'])\n",
        "    y = df['class']\n",
        "\n",
        "    return X, y, vectorizer, cleaner\n",
        "\n",
        "\n",
        "# Train Model\n",
        "\n",
        "def train_and_evaluate(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=y, random_state=42\n",
        "    )\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Prediction\n",
        "\n",
        "def predict(text, model, vectorizer, cleaner):\n",
        "    clean_text = cleaner.clean_text(text)\n",
        "    X = vectorizer.transform([clean_text])\n",
        "    pred = model.predict(X)[0]\n",
        "    proba = model.predict_proba(X).max()\n",
        "\n",
        "    labels = {0: \"Hate Speech\", 1: \"Offensive\", 2: \"Neither\"}\n",
        "    return {\n",
        "        \"label\": labels[pred],\n",
        "        \"confidence\": float(proba)\n",
        "    }\n",
        "\n",
        "\n",
        "# Main Execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Load data\n",
        "    X, y, vectorizer, cleaner = load_and_prepare_data(\"labeled_data.csv\")\n",
        "\n",
        "    model = train_and_evaluate(X, y)\n",
        "\n",
        "    sample_texts = [\n",
        "        \"I hope you have a great day!\",\n",
        "        \"Let’s meet up for lunch tomorrow.\",\n",
        "        \"You're such an idiot, get lost!\",\n",
        "        \"Shut up already, no one cares!\",\n",
        "        \"All [ethnic group] are criminals!\",\n",
        "        \"I hate all [religion] people, they're disgusting.\",\n",
        "        \"People like you ruin this country!\",\n",
        "        \"Congratulations on your achievement!\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for text in sample_texts:\n",
        "        print(f\"Text: {text} -> Prediction: {predict(text, model, vectorizer, cleaner)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save model and vectorizer\n",
        "with open(\"hate_speech_model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n",
        "with open(\"tfidf_vectorizer.pkl\", \"wb\") as vec_file:\n",
        "    pickle.dump(vectorizer, vec_file)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"hate_speech_model.pkl\")\n",
        "files.download(\"tfidf_vectorizer.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0_-D_EzySg6Y",
        "outputId": "ab5df9c5-8992-4e3e-93d4-7e303ba254e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aa010dee-2fb2-488b-9be7-6f5b4a77ac48\", \"hate_speech_model.pkl\", 120764)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5db0115b-46bc-4d85-b4d6-aa4ecaa900b3\", \"tfidf_vectorizer.pkl\", 185218)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}